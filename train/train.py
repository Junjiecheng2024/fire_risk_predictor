# improved_train.py

import os, sys, math
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix

class WeightedFocalLoss(nn.Module):
    def __init__(self, alpha=0.25, gamma=2.0, pos_weight=None, reduction='mean'):
        super(WeightedFocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.pos_weight = pos_weight
        self.reduction = reduction

    def forward(self, inputs, targets):
        # ä½¿ç”¨å¸¦æƒé‡çš„BCE loss
        bce_loss = nn.functional.binary_cross_entropy_with_logits(
            inputs, targets, pos_weight=self.pos_weight, reduction='none'
        )
        prob = torch.sigmoid(inputs)
        pt = prob * targets + (1 - prob) * (1 - targets)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss

        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

# --------------- è·¯å¾„ä¸æ¨¡å‹å¯¼å…¥ ----------------
ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.append(ROOT)
from models.lstm_model import LSTMModel

# --------------- æ•°æ®åŠ è½½ä¸åˆ†æ ----------------
X = np.load(os.path.join(ROOT, "data/data/X_final.npy"))
y = np.load(os.path.join(ROOT, "data/data/y_final.npy"))[:len(X)]

# äºŒå€¼åŒ–
y = (y > 0).astype(np.float32)

# æ•°æ®åˆ†æ
print("=== æ•°æ®åˆ†æ ===")
print(f"æ€»æ ·æœ¬æ•°: {len(y)}")
print(f"æ­£æ ·æœ¬æ•°: {y.sum():.0f} ({y.mean()*100:.2f}%)")
print(f"è´Ÿæ ·æœ¬æ•°: {len(y) - y.sum():.0f} ({(1-y.mean())*100:.2f}%)")
print(f"æ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")

# å¦‚æœæ­£æ ·æœ¬å¤ªå°‘ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´ç­–ç•¥
if y.mean() < 0.01:
    print("âš ï¸  è­¦å‘Š: æ­£æ ·æœ¬æ¯”ä¾‹æä½ï¼Œå»ºè®®è€ƒè™‘å…¶ä»–æ–¹æ³•æˆ–è°ƒæ•´é˜ˆå€¼")

# --------------- è®­ç»ƒå‚æ•° ----------------
n_splits = 5
epochs = 100
batch_size = 128
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ============= å·¥å…·å‡½æ•° =============
def get_loader(X_arr, y_arr, bs, shuffle=True):
    ds = TensorDataset(torch.tensor(X_arr, dtype=torch.float32),
                       torch.tensor(y_arr, dtype=torch.float32))
    return DataLoader(ds, batch_size=bs, shuffle=shuffle, drop_last=False)

def train_epoch(model, loader, loss_fn, opt):
    model.train()
    total, running = 0, 0.0
    all_preds, all_targets = [], []
    
    for xb, yb in loader:
        xb, yb = xb.to(device), yb.to(device)
        opt.zero_grad()
        logits = model(xb).squeeze()
        loss = loss_fn(logits, yb)
        loss.backward()
        opt.step()
        
        running += loss.item() * len(xb)
        total += len(xb)
        
        # æ”¶é›†é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾ç”¨äºåˆ†æ
        with torch.no_grad():
            preds = torch.sigmoid(logits)
            all_preds.append(preds.cpu().numpy())
            all_targets.append(yb.cpu().numpy())
    
    all_preds = np.concatenate(all_preds)
    all_targets = np.concatenate(all_targets)
    
    return running / total, all_preds, all_targets

def eval_epoch(model, loader, loss_fn):
    model.eval()
    y_true, y_prob = [], []
    running, total = 0.0, 0
    
    with torch.no_grad():
        for xb, yb in loader:
            xb, yb = xb.to(device), yb.to(device)
            logits = model(xb).squeeze()
            prob = torch.sigmoid(logits)
            loss = loss_fn(logits, yb)
            
            running += loss.item() * len(xb)
            total += len(xb)
            y_true.append(yb.cpu().numpy())
            y_prob.append(prob.cpu().numpy())
    
    y_true = np.concatenate(y_true)
    y_prob = np.concatenate(y_prob)
    
    # è®¡ç®—AUC
    auc = roc_auc_score(y_true, y_prob) if len(np.unique(y_true)) > 1 else float("nan")
    
    return running / total, auc, y_true, y_prob

# ============= K-fold è®­ç»ƒ =============
skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

all_aucs = []

for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):
    print(f"\nğŸŸ¡ === Fold {fold}/{n_splits} ===")
    
    X_tr, y_tr = X[train_idx], y[train_idx]
    X_te, y_te = X[test_idx], y[test_idx]
    
    # åˆ†ææœ¬æŠ˜æ•°æ®åˆ†å¸ƒ
    print(f"è®­ç»ƒé›†: æ­£æ ·æœ¬ {y_tr.sum():.0f}/{len(y_tr)} ({y_tr.mean()*100:.2f}%)")
    print(f"æµ‹è¯•é›†: æ­£æ ·æœ¬ {y_te.sum():.0f}/{len(y_te)} ({y_te.mean()*100:.2f}%)")
    
    # DataLoader
    train_loader = get_loader(X_tr, y_tr, batch_size, shuffle=True)
    test_loader = get_loader(X_te, y_te, batch_size, shuffle=False)
    
    # è®¡ç®—pos_weight
    pos = y_tr.sum()
    neg = len(y_tr) - pos
    pos_weight = torch.tensor([neg/pos if pos > 0 else 1.0], device=device)
    print(f"pos_weight: {pos_weight.item():.4f}")
    
    # æ¨¡å‹ & ä¼˜åŒ–å™¨ & æŸå¤±å‡½æ•°
    model = LSTMModel(X.shape[2], hidden_dim=64, num_layers=2, bidir=True).to(device)
    
    # å°è¯•ä¸åŒçš„æŸå¤±å‡½æ•°
    use_focal = True  # å¯ä»¥æ”¹ä¸ºFalseä½¿ç”¨BCE
    if use_focal:
        loss_fn = WeightedFocalLoss(alpha=0.25, gamma=2.0, pos_weight=pos_weight)
    else:
        loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    
    # ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
    opt = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        opt, mode='min', factor=0.5, patience=10
    )
    
    best_auc = 0.0
    patience_counter = 0
    patience = 20
    current_lr = opt.param_groups[0]['lr']
    
    # -------- è®­ç»ƒå¾ªç¯ ----------
    for epoch in range(1, epochs + 1):
        tr_loss, tr_preds, tr_targets = train_epoch(model, train_loader, loss_fn, opt)
        te_loss, auc, te_targets, te_preds = eval_epoch(model, test_loader, loss_fn)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        old_lr = opt.param_groups[0]['lr']
        scheduler.step(te_loss)
        new_lr = opt.param_groups[0]['lr']
        
        # å¦‚æœå­¦ä¹ ç‡å‘ç”Ÿå˜åŒ–ï¼Œæ‰“å°ä¿¡æ¯
        if new_lr != old_lr:
            print(f"  å­¦ä¹ ç‡ä» {old_lr:.6f} é™ä½åˆ° {new_lr:.6f}")
        
        # æ—©åœ
        if auc > best_auc:
            best_auc = auc
            patience_counter = 0
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            best_model_path = os.path.join(ROOT, f"models/best_lstm_fold{fold}.pt")
            os.makedirs(os.path.dirname(best_model_path), exist_ok=True)
            torch.save(model.state_dict(), best_model_path)
        else:
            patience_counter += 1
        
        # æ‰“å°è®­ç»ƒä¿¡æ¯
        auc_str = f"{auc:.4f}" if not math.isnan(auc) else "nan"
        print(f"Epoch {epoch:3d}: Train Loss={tr_loss:.4f}, Test Loss={te_loss:.4f}, "
              f"AUC={auc_str}, Best AUC={best_auc:.4f}, LR={new_lr:.6f}")
        
        # æ¯10ä¸ªepochè¯¦ç»†åˆ†æä¸€æ¬¡
        if epoch % 10 == 0:
            # åˆ†æé¢„æµ‹åˆ†å¸ƒ
            print(f"  è®­ç»ƒé›†é¢„æµ‹åˆ†å¸ƒ: min={tr_preds.min():.4f}, max={tr_preds.max():.4f}, "
                  f"mean={tr_preds.mean():.4f}, std={tr_preds.std():.4f}")
            print(f"  æµ‹è¯•é›†é¢„æµ‹åˆ†å¸ƒ: min={te_preds.min():.4f}, max={te_preds.max():.4f}, "
                  f"mean={te_preds.mean():.4f}, std={te_preds.std():.4f}")
            
            # åˆ†æé¢„æµ‹é˜ˆå€¼
            thresholds = [0.1, 0.3, 0.5, 0.7, 0.9]
            for thresh in thresholds:
                pred_binary = (te_preds > thresh).astype(int)
                tp = ((pred_binary == 1) & (te_targets == 1)).sum()
                fp = ((pred_binary == 1) & (te_targets == 0)).sum()
                tn = ((pred_binary == 0) & (te_targets == 0)).sum()
                fn = ((pred_binary == 0) & (te_targets == 1)).sum()
                
                precision = tp / (tp + fp) if (tp + fp) > 0 else 0
                recall = tp / (tp + fn) if (tp + fn) > 0 else 0
                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
                
                print(f"    é˜ˆå€¼{thresh}: Precision={precision:.3f}, Recall={recall:.3f}, F1={f1:.3f}")
        
        # æ—©åœæ£€æŸ¥
        if patience_counter >= patience:
            print(f"æ—©åœè§¦å‘ï¼Œåœ¨epoch {epoch}")
            break
    
    all_aucs.append(best_auc)
    print(f"âœ… Fold {fold} å®Œæˆï¼Œæœ€ä½³ AUC: {best_auc:.4f}")

# æ€»ç»“
print(f"\nğŸ === è®­ç»ƒå®Œæˆ ===")
print(f"å„æŠ˜AUC: {[f'{auc:.4f}' for auc in all_aucs]}")
print(f"å¹³å‡AUC: {np.mean(all_aucs):.4f} Â± {np.std(all_aucs):.4f}")

# å¦‚æœAUCè¿˜æ˜¯å¾ˆä½ï¼Œç»™å‡ºå»ºè®®
if np.mean(all_aucs) < 0.6:
    print("\nâš ï¸  AUCä»ç„¶è¾ƒä½ï¼Œå»ºè®®å°è¯•ä»¥ä¸‹æ–¹æ³•:")
    print("1. æ£€æŸ¥æ•°æ®è´¨é‡å’Œç‰¹å¾å·¥ç¨‹")
    print("2. å°è¯•ä¸åŒçš„æ¨¡å‹æ¶æ„ (CNN, Transformer)")
    print("3. è°ƒæ•´æ•°æ®é¢„å¤„ç†æ–¹å¼")
    print("4. è€ƒè™‘ä½¿ç”¨æ›´å¤æ‚çš„é‡‡æ ·ç­–ç•¥")
    print("5. æ£€æŸ¥æ ‡ç­¾æ˜¯å¦æ­£ç¡®")